{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sunbert_v2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM2NH6qxHfuYGgYlZROZQj3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAtocxILn9ob","executionInfo":{"status":"ok","timestamp":1606938535115,"user_tz":300,"elapsed":5291,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"1903da4b-c7ae-47e0-f7f4-75148ee184e3"},"source":["!pip install -q -U watermark \n","!pip install -qq transformers==3.5.1\n","%reload_ext watermark\n","%watermark -v -p numpy,pandas,torch,transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPython 3.6.9\n","IPython 5.5.0\n","\n","numpy 1.18.5\n","pandas 1.1.4\n","torch 1.7.0+cu101\n","transformers 3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-IfIc8vLfgJ","executionInfo":{"status":"ok","timestamp":1606938535293,"user_tz":300,"elapsed":3404,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"8ed9c0cb-2d33-4613-8784-333c4ee5a15d"},"source":["## Making sure we have a GPU at our disposal ðŸ‘Œ\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Dec  2 19:48:56 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   74C    P0    34W /  70W |   4703MiB / 15079MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ir0QDuemLnv3","executionInfo":{"status":"ok","timestamp":1606938535294,"user_tz":300,"elapsed":2027,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"83eb8ebc-777c-4da7-b6ba-cd97cbf9a73c"},"source":["import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","transformers.logging.set_verbosity_info()\n","transformers.logging.CRITICAL\n","\n","import numpy as np \n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt \n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap \n","\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","%matplotlib inline \n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","\n","rcParams['figure.figsize'] = 12, 8\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":180}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0uTFZLpMB19","executionInfo":{"status":"ok","timestamp":1606938536713,"user_tz":300,"elapsed":407,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"79b6bfab-946c-44f5-8a6f-84965d18a175"},"source":["## Loading, Concatenating and Resampling Dataset\n","\n","!mkdir data"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory â€˜dataâ€™: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3iVLrwkwMmEl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606935423048,"user_tz":300,"elapsed":10200,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"344852fb-2a8a-463f-c19e-1ef1d5815d7d"},"source":["!gdown --id 1xZSOCiUQRR4Sw3SpumLMU-3i66BAxZ5u\n","!gdown --id 1Rzy2HKsjgnt3WO_2HhWgZP2Bc3lELEIc\n","!gdown --id 1-q6ubR60q3cdgS_8xvTHztjc4bLopbR1\n","\n","!mv *.csv data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1xZSOCiUQRR4Sw3SpumLMU-3i66BAxZ5u\n","To: /content/CrowdTangle-covid-non-covid-posts.csv\n","100% 671k/671k [00:00<00:00, 108MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1Rzy2HKsjgnt3WO_2HhWgZP2Bc3lELEIc\n","To: /content/mohengagers-annotated.csv\n","100% 376k/376k [00:00<00:00, 11.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1-q6ubR60q3cdgS_8xvTHztjc4bLopbR1\n","To: /content/moh-annotated.csv\n","100% 62.8k/62.8k [00:00<00:00, 54.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0fBYdBqvLv-","executionInfo":{"status":"ok","timestamp":1606935423203,"user_tz":300,"elapsed":10345,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"b8108b96-c639-40bb-e31f-e0940393ad75"},"source":["!mv *.csv data/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mv: cannot stat '*.csv': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xFvPkOHWwgeP"},"source":["import glob\n","path = 'data/'\n","all_files = glob.glob(path + '/*.csv')\n","li = []\n","\n","for filename in all_files:\n","  df = pd.read_csv(filename, index_col=None, header=0)\n","  li.append(df)\n","\n","frame = pd.concat(li, axis = 0, ignore_index = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLW1vDR7xEgo","executionInfo":{"status":"ok","timestamp":1606938865825,"user_tz":300,"elapsed":909,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"e4de1f02-caa0-4ad4-f773-bdeeed0c39ed"},"source":["frame.head(6)\n","frame.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(24199, 4)"]},"metadata":{"tags":[]},"execution_count":184}]},{"cell_type":"code","metadata":{"id":"dwB2uvdOxGp-"},"source":["def Groupby_OneCol_comp_plot(df, col, plt_style = 'seaborn-ticks', color_palette = \"coolwarm\"):\n","    '''\n","    Group by col1, sort by size , return and plot the dataframe with a bar and pie plot\n","    '''\n","    gr=pd.DataFrame()\n","    gr['{} No'.format(col)] = df.groupby(col).size()\n","    gr['{} Ratio'.format(col)] = np.round(gr['{} No'.format(col)].divide(gr['{} No'.format(col)].sum())*100,0)\n","    \n","    print ('Total No. of {}:{}'.format(col,gr['{} No'.format(col)].sum()))\n","    \n","    plt.style.use(plt_style)\n","    sns.set_palette(sns.color_palette(color_palette))\n","    \n","    fig=plt.figure()\n","    plt.axis('off')\n","\n","    fig.add_subplot(121)\n","    \n","    ax=gr['{} No'.format(col)].plot(kind='bar', title='{} Counts'.format(col), figsize=(16,8), color=sns.color_palette())\n","    _ = plt.setp(ax.get_xticklabels(), rotation=0)\n","    for p in ax.patches: ax.annotate(np.round(p.get_height(),decimals=2),\n","                                     (p.get_x()+p.get_width()/2., p.get_height()),\n","                                     ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n","    ax.get_yaxis().set_ticks([])\n","    plt.xlabel('')\n","\n","    fig.add_subplot(122)\n","    plt.axis('off')\n","    gr.loc[:,'{} Ratio'.format(col)].plot(kind= 'pie',\n","                                     autopct='%1.1f%%',shadow=False,\n","                                     title='{} Ratio'.format(col), legend=False, labels=None);\n","    sns.despine(top=True, right=True, left=True, bottom=False);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"hF_OmoODxdYs","executionInfo":{"status":"ok","timestamp":1606938869676,"user_tz":300,"elapsed":827,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"b3f85102-f0b4-4ed0-b2de-93357b5ddb70"},"source":["Groupby_OneCol_comp_plot(frame, 'category')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total No. of category:24199\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x576 with 3 Axes>"]},"metadata":{"tags":[],"image/png":{"width":907,"height":487}}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630},"id":"S61fKCQ5xg25","executionInfo":{"status":"ok","timestamp":1606938869983,"user_tz":300,"elapsed":887,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"2e656629-71cf-4f8f-9e01-dd8252a86ac3"},"source":["## Sampling dataset\n","\n","count_class_0, count_class_1 = frame.category.value_counts()\n","df_class_0 = frame[frame['category']=='Non-COVID']\n","df_class_1 = frame[frame['category']=='COVID']\n","\n","\n","## Random Over sampling\n","\n","df_class_0_under = df_class_0.sample(count_class_1)\n","df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n","\n","print('Random under-sampling:')\n","print(df_test_under.category.value_counts())\n","\n","df_test_under.category.value_counts().plot(kind='bar', title='Count (target)');"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random under-sampling:\n","COVID        1138\n","Non-COVID    1138\n","Name: category, dtype: int64\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[],"image/png":{"width":722,"height":545}}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"jzVMZIbqxv1f","executionInfo":{"status":"ok","timestamp":1606938871104,"user_tz":300,"elapsed":1806,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"d44edf09-a917-4581-ff66-e0f1b8f1a768"},"source":["Groupby_OneCol_comp_plot(df_test_under, 'category')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total No. of category:2276\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1152x576 with 3 Axes>"]},"metadata":{"tags":[],"image/png":{"width":907,"height":487}}}]},{"cell_type":"code","metadata":{"id":"mh_gN7nCx0Zu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0reW3JOeJcR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9_fXr5Yuyfbh"},"source":["##Data Prep"]},{"cell_type":"code","metadata":{"id":"YLkrBUkpzq41"},"source":["df_test_under.category = df_test_under.category.apply(lambda x: 0 if x==\"Non-COVID\" else 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDEgWyNTeKpp"},"source":["sample_text = \"Ugandans should know that COVID-19 is real\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tyf7zqBFyg0m"},"source":["PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3U4YNmpypr5","executionInfo":{"status":"ok","timestamp":1606938876147,"user_tz":300,"elapsed":710,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"8df28d84-9e28-4938-eb62-1c43dd67bf2b"},"source":["encoding = tokenizer.encode_plus(\n","    sample_text,\n","    max_length = 32,\n","    add_special_tokens = True,\n","    return_token_type_ids = False,\n","    pad_to_max_length = True,\n","    padding=True,\n","    return_attention_mask = True,\n","    truncation = True,\n","    return_tensors = 'pt'\n","\n",")\n","\n","encoding.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask'])"]},"metadata":{"tags":[]},"execution_count":192}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DJk8T9Ezowa","executionInfo":{"status":"ok","timestamp":1606938876148,"user_tz":300,"elapsed":547,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"6f0ac9ed-0304-4a3c-f63e-60344b47de56"},"source":["## Sanity check for input ids and attention masks\n","print(len(encoding['input_ids'][0]))\n","encoding['input_ids'][0]\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([  101,  9831,  2316,  1431,  1221,  1115, 18732, 23314,  2137,   118,\n","         1627,  1110,  1842,   102])"]},"metadata":{"tags":[]},"execution_count":193}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":575},"id":"pQH37EpG0WBD","executionInfo":{"status":"ok","timestamp":1606938877240,"user_tz":300,"elapsed":853,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"648a3de8-b274-477f-ff36-0e7d4d2c5c8b"},"source":["## choosing the right sequence length\n","\n","token_lens = []\n","\n","for txt in df_test_under:\n","  tokens = tokenizer.encode(txt, max_length=512)\n","  token_lens.append(len(tokens))\n","\n","sns.distplot(token_lens)\n","plt.xlim([0,60]);\n","plt.xlabel('Token Count')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Token Count')"]},"metadata":{"tags":[]},"execution_count":194},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[],"image/png":{"width":737,"height":488}}}]},{"cell_type":"code","metadata":{"id":"ITWy7N3D1CSi"},"source":["## From the graph, I will choose a max len as 20, since most of the tokens are short\n","## choose the appropriate length based on the dataset in question\n","MAX_LEN = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKMToXuR1uKC"},"source":["\n","## Creating the dataset class\n","\n","class SunbertDataset():\n","  \n","  def __init__(self, text, targets, tokenizer, max_len):\n","    self.text = text\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.text)\n","\n","  def __getitem__(self, item):\n","    text = str(self.text[item])\n","    target = self.targets[item]\n","\n","    encoding = self.tokenizer.encode_plus(\n","        text,\n","        add_special_tokens = True,\n","        max_length = self.max_len,\n","        # padding='longest',\n","        return_token_type_ids = False,\n","        pad_to_max_length = True,\n","        return_tensors = 'pt'\n","    )\n","\n","    return {\n","        'input_text': text,\n","        'input_ids': encoding['input_ids'].flatten(),\n","        'attention_mask': encoding['attention_mask'].flatten(),\n","        'targets': torch.tensor(target, dtype = torch.long)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnVA3W9vY8U5"},"source":["df_train, df_test = train_test_split(df_test_under, test_size = 0.1, random_state = RANDOM_SEED)\n","df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"id":"ttpmgEeue_sZ","executionInfo":{"status":"ok","timestamp":1606938880336,"user_tz":300,"elapsed":282,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"cc62b84d-03c6-427b-cddd-591dbdb80c66"},"source":["# Sanity Check: Is the data okay?\n","df_train.head(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>category</th>\n","      <th>Unnamed: 0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9418</th>\n","      <td>102852</td>\n","      <td>â€œ our cardinal role as medical workers is to s...</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9166</th>\n","      <td>102600</td>\n","      <td>rt at_user it 's because of dedicated doctors ...</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2393</th>\n","      <td>9634</td>\n","      <td>Despite the COVID 19 Pandemic threat across th...</td>\n","      <td>1</td>\n","      <td>645.0</td>\n","    </tr>\n","    <tr>\n","      <th>4640</th>\n","      <td>12244</td>\n","      <td>Lockdown: One womanâ€™s plea with Museveni over ...</td>\n","      <td>1</td>\n","      <td>3259.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  ... Unnamed: 0\n","9418  102852  ...        NaN\n","9166  102600  ...        NaN\n","2393    9634  ...      645.0\n","4640   12244  ...     3259.0\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":198}]},{"cell_type":"code","metadata":{"id":"D_xlhZ-SZ4PZ"},"source":["def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = SunbertDataset(\n","      text = df.text.to_numpy(),\n","      targets = df.category.to_numpy(),\n","      tokenizer = tokenizer,\n","      max_len = max_len\n","  )\n","\n","  return DataLoader(\n","      ds,\n","      batch_size=batch_size,\n","      num_workers = 4\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M89M4EMrbxoO"},"source":["BATCH_SIZE = 16\n","\n","train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukH5rCR0cKyX","executionInfo":{"status":"ok","timestamp":1606938887698,"user_tz":300,"elapsed":578,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"89cbc81a-8600-4f9c-e8a0-11c5429159ac"},"source":["## Sanity check\n","data = next(iter(train_data_loader))\n","data.keys()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_text', 'input_ids', 'attention_mask', 'targets'])"]},"metadata":{"tags":[]},"execution_count":201}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkYwNGY5fwdi","executionInfo":{"status":"ok","timestamp":1606938888425,"user_tz":300,"elapsed":274,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"c71d8cd8-135f-4f59-dc46-22ac5753ed9d"},"source":["print(data['input_ids'].shape)\n","print(data['attention_mask'].shape)\n","print(data['targets'].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([16, 20])\n","torch.Size([16, 20])\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7Upb0CRyPLSA"},"source":["## HuggingFace ðŸ¤— + SunBert  "]},{"cell_type":"code","metadata":{"id":"C0niIIxKgOWJ"},"source":["bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaLIxj0uga7H"},"source":["class SunbertClassifier(nn.Module):\n","  def __init__(self, n_classes):\n","    super(SunbertClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert(\n","        input_ids = input_ids,\n","        attention_mask = attention_mask\n","    )\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"poV0lIHPgrfp"},"source":["model = SunbertClassifier(2)\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rldsmA1uguiT","executionInfo":{"status":"ok","timestamp":1606938899891,"user_tz":300,"elapsed":214,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"47c7b0d0-e87e-471a-f558-c4ed728d1c62"},"source":["# Moving the training data to GPU as well\n","\n","input_ids = data['input_ids'].to(device)\n","attention_mask = data['attention_mask'].to(device)\n","\n","print(input_ids.shape) # -> batch_size X seq length\n","print(attention_mask.shape) # -> batch_size x Seq length"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([16, 20])\n","torch.Size([16, 20])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyG7hgWKi7o8","executionInfo":{"status":"ok","timestamp":1606938901728,"user_tz":300,"elapsed":211,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"8443895f-8660-4d5d-de9a-30177cf9c1e5"},"source":["## Applying softmax to get the predicted probabilities\n","F.softmax(model(input_ids, attention_mask), dim=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6425, 0.3575],\n","        [0.7583, 0.2417],\n","        [0.6298, 0.3702],\n","        [0.8030, 0.1970],\n","        [0.7650, 0.2350],\n","        [0.6338, 0.3662],\n","        [0.7576, 0.2424],\n","        [0.5269, 0.4731],\n","        [0.6802, 0.3198],\n","        [0.5212, 0.4788],\n","        [0.7021, 0.2979],\n","        [0.6443, 0.3557],\n","        [0.8663, 0.1337],\n","        [0.7748, 0.2252],\n","        [0.6606, 0.3394],\n","        [0.6091, 0.3909]], device='cuda:0', grad_fn=<SoftmaxBackward>)"]},"metadata":{"tags":[]},"execution_count":207}]},{"cell_type":"code","metadata":{"id":"8mypump5jdJ2"},"source":["EPOCHS = 200\n","\n","optimizer = AdamW(model.parameters(), lr = 2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YqYI1KCklRC"},"source":["def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for d in data_loader:\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(preds == targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVmHCQU4qp4K"},"source":["## Eval \n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","          \n","          input_ids = input_ids,\n","          attention_mask = attention_mask\n","      )\n","\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double()/n_examples, np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awk8G0uL8IE9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3LoKDUb8anv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bflMlEbvs8b_","executionInfo":{"status":"ok","timestamp":1606941652900,"user_tz":300,"elapsed":1690396,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"3265be3a-c4e4-4895-d907-22b6956925c2"},"source":["%%time\n","\n","history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(df_train)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device, \n","    len(df_val)\n","  )\n","\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","\n","  if val_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = val_acc"],"execution_count":211,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.503682995447889 accuracy 0.7451171875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 0.39423301769420505 accuracy 0.763157894736842\n","\n","Epoch 2/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.294220918265637 accuracy 0.90234375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 0.5224040262401104 accuracy 0.7719298245614035\n","\n","Epoch 3/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.14715664231152914 accuracy 0.95751953125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 0.7327954536303878 accuracy 0.7894736842105263\n","\n","Epoch 4/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.07815597885837633 accuracy 0.97900390625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.13155571045354 accuracy 0.7807017543859649\n","\n","Epoch 5/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.06640949810116581 accuracy 0.98388671875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 0.7823816891759634 accuracy 0.8245614035087718\n","\n","Epoch 6/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.03086075166925184 accuracy 0.99169921875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.143634770065546 accuracy 0.763157894736842\n","\n","Epoch 7/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.016258736936038076 accuracy 0.99609375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.216939533213008 accuracy 0.763157894736842\n","\n","Epoch 8/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.024059501151498353 accuracy 0.9951171875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.2836473624120117 accuracy 0.7807017543859649\n","\n","Epoch 9/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.011786349871783841 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.1041441038250923 accuracy 0.7368421052631579\n","\n","Epoch 10/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010152828362151922 accuracy 0.9970703125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6974526904523373 accuracy 0.7543859649122806\n","\n","Epoch 11/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010519951598411126 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7300522029399872 accuracy 0.7456140350877193\n","\n","Epoch 12/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006366800435756659 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5836074453909532 accuracy 0.7719298245614035\n","\n","Epoch 13/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008402333464843537 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.615579410627106 accuracy 0.7719298245614035\n","\n","Epoch 14/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004846831412223196 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6765205112787953 accuracy 0.763157894736842\n","\n","Epoch 15/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008252870959481129 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.586667441573809 accuracy 0.7719298245614035\n","\n","Epoch 16/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006025321287737029 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.657046891193204 accuracy 0.763157894736842\n","\n","Epoch 17/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010887090712976999 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.681137129491617 accuracy 0.7719298245614035\n","\n","Epoch 18/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.007041214848051425 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7003357707817486 accuracy 0.7807017543859649\n","\n","Epoch 19/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00462551915575915 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8822469841688871 accuracy 0.7719298245614035\n","\n","Epoch 20/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00888234595899462 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7741487845455595 accuracy 0.763157894736842\n","\n","Epoch 21/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.011588277363294708 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6356874704044913 accuracy 0.8070175438596491\n","\n","Epoch 22/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008464367714680066 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.265296183526516 accuracy 0.7543859649122806\n","\n","Epoch 23/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004186509440607722 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.778668776154518 accuracy 0.7280701754385964\n","\n","Epoch 24/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.013997530037055839 accuracy 0.9970703125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.656712501993752 accuracy 0.763157894736842\n","\n","Epoch 25/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0048153009381479706 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6649768277969201 accuracy 0.7719298245614035\n","\n","Epoch 26/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004723893178976368 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7790062427498583 accuracy 0.7719298245614035\n","\n","Epoch 27/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004414394603958982 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8457650914771193 accuracy 0.763157894736842\n","\n","Epoch 28/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004342353441259661 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.1068817153554846 accuracy 0.7543859649122806\n","\n","Epoch 29/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004207804088178335 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.123299196361188 accuracy 0.7543859649122806\n","\n","Epoch 30/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004432434505996952 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.1301784515370628 accuracy 0.7368421052631579\n","\n","Epoch 31/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003459318252918564 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.1906672641625846 accuracy 0.7368421052631579\n","\n","Epoch 32/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004823435500210138 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.2461028397075893 accuracy 0.7280701754385964\n","\n","Epoch 33/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.005540260396294627 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.2121779471628145 accuracy 0.7280701754385964\n","\n","Epoch 34/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004250184384034128 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.1839014142744873 accuracy 0.7280701754385964\n","\n","Epoch 35/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004222932446189986 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.28085598349503 accuracy 0.7280701754385964\n","\n","Epoch 36/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004239336375503555 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.0709284171456375 accuracy 0.7368421052631579\n","\n","Epoch 37/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0037274842393060226 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.203657373785404 accuracy 0.7456140350877193\n","\n","Epoch 38/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003973724700715664 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.310525268315871 accuracy 0.7368421052631579\n","\n","Epoch 39/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.01817160223069436 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8553274596924894 accuracy 0.7456140350877193\n","\n","Epoch 40/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004006155953234725 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9700032843456938 accuracy 0.7456140350877193\n","\n","Epoch 41/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004218946425154968 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9788904263623408 accuracy 0.7456140350877193\n","\n","Epoch 42/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008886194416113824 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.46121253038109 accuracy 0.8157894736842105\n","\n","Epoch 43/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0072582752260359484 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8744922429025337 accuracy 0.7719298245614035\n","\n","Epoch 44/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.009016948573332684 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.0563831774193204 accuracy 0.7368421052631579\n","\n","Epoch 45/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010540352231615202 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.778888046691236 accuracy 0.7719298245614035\n","\n","Epoch 46/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.01929672452746445 accuracy 0.99609375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8865836489003414 accuracy 0.7543859649122806\n","\n","Epoch 47/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.019524703016891465 accuracy 0.99560546875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.577130415225838 accuracy 0.7807017543859649\n","\n","Epoch 48/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.019675409485415685 accuracy 0.99609375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.328333689943065 accuracy 0.7894736842105263\n","\n","Epoch 49/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.01818923129292216 accuracy 0.99560546875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.2160093454622256 accuracy 0.7807017543859649\n","\n","Epoch 50/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010948355990379355 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.4908718722508638 accuracy 0.7894736842105263\n","\n","Epoch 51/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.009743940665096318 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5866001165231864 accuracy 0.763157894736842\n","\n","Epoch 52/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.005156537602516664 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.4894916190983167 accuracy 0.7894736842105263\n","\n","Epoch 53/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0041225455048419235 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.77747103562524 accuracy 0.763157894736842\n","\n","Epoch 54/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008614529057389575 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.416714757680893 accuracy 0.7456140350877193\n","\n","Epoch 55/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.010549494062473741 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.010231703519821 accuracy 0.763157894736842\n","\n","Epoch 56/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006174274512297018 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8038125485181808 accuracy 0.7807017543859649\n","\n","Epoch 57/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003329619433802833 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6966151879169047 accuracy 0.7894736842105263\n","\n","Epoch 58/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003567877456234214 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6981372980371816 accuracy 0.7894736842105263\n","\n","Epoch 59/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036173194306670098 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7206111932173371 accuracy 0.7894736842105263\n","\n","Epoch 60/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003960241360346117 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6933995023482566 accuracy 0.7894736842105263\n","\n","Epoch 61/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003716044855945455 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.718148042788016 accuracy 0.7894736842105263\n","\n","Epoch 62/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0046045025056713484 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7250685570070345 accuracy 0.7894736842105263\n","\n","Epoch 63/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003213271731691947 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7715294640456705 accuracy 0.7807017543859649\n","\n","Epoch 64/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0037937788611586143 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.756936698971458 accuracy 0.7894736842105263\n","\n","Epoch 65/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003108848617756621 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7760632334916409 accuracy 0.7894736842105263\n","\n","Epoch 66/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00362763222722684 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8258660957167763 accuracy 0.7982456140350876\n","\n","Epoch 67/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033992455222762175 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8543017953589356 accuracy 0.7894736842105263\n","\n","Epoch 68/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003921573468614881 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8784026503510631 accuracy 0.7894736842105263\n","\n","Epoch 69/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003558854696381175 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8961559608512744 accuracy 0.7894736842105263\n","\n","Epoch 70/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0038683058621202804 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8991586267870844 accuracy 0.7894736842105263\n","\n","Epoch 71/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0028814149886891016 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9989386602537706 accuracy 0.7807017543859649\n","\n","Epoch 72/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035832635526809042 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.080048255622387 accuracy 0.763157894736842\n","\n","Epoch 73/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035201565831073367 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8619306906357451 accuracy 0.7894736842105263\n","\n","Epoch 74/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032920880442759426 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.834780804509137 accuracy 0.7982456140350876\n","\n","Epoch 75/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00348428113263477 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.984856501221657 accuracy 0.7543859649122806\n","\n","Epoch 76/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.015592267338012888 accuracy 0.9970703125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7041673213000195 accuracy 0.7982456140350876\n","\n","Epoch 77/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.01447189785591263 accuracy 0.9970703125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.163075402379036 accuracy 0.7894736842105263\n","\n","Epoch 78/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.007640803660521556 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.978464551270008 accuracy 0.7894736842105263\n","\n","Epoch 79/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003233527051648366 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.174507036805153 accuracy 0.7807017543859649\n","\n","Epoch 80/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033885640113027193 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.0608889907598495 accuracy 0.7807017543859649\n","\n","Epoch 81/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003148171244667708 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.079365387558937 accuracy 0.7894736842105263\n","\n","Epoch 82/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003393754157555051 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.214359685778618 accuracy 0.7894736842105263\n","\n","Epoch 83/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033781306307734127 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.2412607818841934 accuracy 0.7894736842105263\n","\n","Epoch 84/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003691758707176973 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.292364001274109 accuracy 0.7807017543859649\n","\n","Epoch 85/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003343906914282435 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.2925698310136795 accuracy 0.7807017543859649\n","\n","Epoch 86/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004085933646532425 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.332704484462738 accuracy 0.7807017543859649\n","\n","Epoch 87/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006069209535167985 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.307613283395767 accuracy 0.763157894736842\n","\n","Epoch 88/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.014288060109489464 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7364953309262319 accuracy 0.7719298245614035\n","\n","Epoch 89/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006793202809816989 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.92259908467463 accuracy 0.7807017543859649\n","\n","Epoch 90/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036637052975390105 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7867014408109014 accuracy 0.8070175438596491\n","\n","Epoch 91/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032624902351763296 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8475016430018059 accuracy 0.7982456140350876\n","\n","Epoch 92/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033955608915317015 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8625100329516044 accuracy 0.7894736842105263\n","\n","Epoch 93/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035459464789049377 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.870112165808493 accuracy 0.7894736842105263\n","\n","Epoch 94/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003966745225602253 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8870530873534932 accuracy 0.7894736842105263\n","\n","Epoch 95/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.002969502091143106 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8932142928240694 accuracy 0.7894736842105263\n","\n","Epoch 96/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036374869218036565 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.906013496219913 accuracy 0.7894736842105263\n","\n","Epoch 97/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0029220150981101867 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8063159808513092 accuracy 0.8245614035087718\n","\n","Epoch 98/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003484869372583077 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.911124289035655 accuracy 0.8245614035087718\n","\n","Epoch 99/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003275215094152628 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.061764694747467 accuracy 0.7807017543859649\n","\n","Epoch 100/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0031853874580285435 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 2.0567983388872335 accuracy 0.7807017543859649\n","\n","Epoch 101/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.017610175374737302 accuracy 0.9970703125\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5571978613686497 accuracy 0.8070175438596491\n","\n","Epoch 102/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035549144043347525 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6640136018386897 accuracy 0.8245614035087718\n","\n","Epoch 103/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0030215944994012034 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6500874087205943 accuracy 0.8070175438596491\n","\n","Epoch 104/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033725567716080818 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6431600525968975 accuracy 0.8157894736842105\n","\n","Epoch 105/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008326501391205277 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7506498992441095 accuracy 0.8070175438596491\n","\n","Epoch 106/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004640604451749297 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5684155151221262 accuracy 0.8070175438596491\n","\n","Epoch 107/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003297069292640842 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6780679561106808 accuracy 0.7982456140350876\n","\n","Epoch 108/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008749284821983672 accuracy 0.99755859375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6155590116721896 accuracy 0.8070175438596491\n","\n","Epoch 109/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004088585932106703 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6229197159259456 accuracy 0.8157894736842105\n","\n","Epoch 110/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00330148687575349 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6586112380007307 accuracy 0.8245614035087718\n","\n","Epoch 111/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.009382927816870001 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.466785311594549 accuracy 0.8157894736842105\n","\n","Epoch 112/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.007754638749183584 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.4468474790683103 accuracy 0.8070175438596491\n","\n","Epoch 113/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0057003297414226495 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5206687482141206 accuracy 0.8070175438596491\n","\n","Epoch 114/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004577331992098976 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5293520524073756 accuracy 0.8070175438596491\n","\n","Epoch 115/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004092926519497553 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.564267500974438 accuracy 0.8070175438596491\n","\n","Epoch 116/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003729038533924367 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5691764502753358 accuracy 0.7982456140350876\n","\n","Epoch 117/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0041929359393506616 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6678862124392708 accuracy 0.7982456140350876\n","\n","Epoch 118/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00314940114580331 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6951718553658566 accuracy 0.7982456140350876\n","\n","Epoch 119/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003333210931263153 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7126192599354226 accuracy 0.7982456140350876\n","\n","Epoch 120/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032735408659743115 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7314286529865512 accuracy 0.7982456140350876\n","\n","Epoch 121/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0029712007438309485 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7437472492310917 accuracy 0.7982456140350876\n","\n","Epoch 122/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035042797256910063 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.748417764890064 accuracy 0.7982456140350876\n","\n","Epoch 123/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0028994884366442264 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7637367546466294 accuracy 0.7982456140350876\n","\n","Epoch 124/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0027604238984979546 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7721315100720858 accuracy 0.7982456140350876\n","\n","Epoch 125/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0031556344569558803 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7865549996438972 accuracy 0.7982456140350876\n","\n","Epoch 126/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0027528587840892627 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8194733411039579 accuracy 0.7982456140350876\n","\n","Epoch 127/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0037907964036856256 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8228809162943094 accuracy 0.7982456140350876\n","\n","Epoch 128/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0026714614930876124 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8853437229986412 accuracy 0.8070175438596491\n","\n","Epoch 129/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.007815572649019664 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.781858578315905 accuracy 0.7982456140350876\n","\n","Epoch 130/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00387225318703166 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7807455137284478 accuracy 0.7982456140350876\n","\n","Epoch 131/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036728950431799134 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9396753832690479 accuracy 0.7894736842105263\n","\n","Epoch 132/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.01043373015088278 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7196268271090958 accuracy 0.7807017543859649\n","\n","Epoch 133/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0064301357949965166 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.593010641856381 accuracy 0.7894736842105263\n","\n","Epoch 134/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.006083078281278631 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.595517492797626 accuracy 0.7894736842105263\n","\n","Epoch 135/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.005051538334342354 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5668562715381995 accuracy 0.8157894736842105\n","\n","Epoch 136/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003748219898284333 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.5870723797447681 accuracy 0.8070175438596491\n","\n","Epoch 137/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004110175733529786 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.608613423944007 accuracy 0.8070175438596491\n","\n","Epoch 138/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00376117830009548 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6129508390231422 accuracy 0.8070175438596491\n","\n","Epoch 139/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004299380371046269 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6345348357615421 accuracy 0.8070175438596491\n","\n","Epoch 140/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004371391826046178 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.643917456239251 accuracy 0.8070175438596491\n","\n","Epoch 141/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003709223971229747 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6461638137457157 accuracy 0.8070175438596491\n","\n","Epoch 142/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003598659644424629 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6504536717897054 accuracy 0.8070175438596491\n","\n","Epoch 143/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036715205585551303 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.672030985342417 accuracy 0.8070175438596491\n","\n","Epoch 144/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0031597062995452063 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6734094321593602 accuracy 0.8070175438596491\n","\n","Epoch 145/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034997550088622376 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6825695633775695 accuracy 0.8070175438596491\n","\n","Epoch 146/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035494910832900572 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6989493071957895 accuracy 0.8070175438596491\n","\n","Epoch 147/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003374073560401314 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7079677805240863 accuracy 0.8070175438596491\n","\n","Epoch 148/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.008149370603038442 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6675297990402669 accuracy 0.8157894736842105\n","\n","Epoch 149/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036484696370546743 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7513935565924612 accuracy 0.7982456140350876\n","\n","Epoch 150/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.002895295567105549 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7535599842647116 accuracy 0.7982456140350876\n","\n","Epoch 151/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032278199585178413 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7889180332406909 accuracy 0.7982456140350876\n","\n","Epoch 152/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035186851493831384 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7937184125171655 accuracy 0.7982456140350876\n","\n","Epoch 153/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004194175630046537 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6986614838227183 accuracy 0.8157894736842105\n","\n","Epoch 154/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003583057483409924 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.6918859705314162 accuracy 0.8157894736842105\n","\n","Epoch 155/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003920056905904801 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7022149786341743 accuracy 0.8157894736842105\n","\n","Epoch 156/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003370387846681311 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7097928747525089 accuracy 0.8157894736842105\n","\n","Epoch 157/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0038093112874975077 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7203885391346319 accuracy 0.8157894736842105\n","\n","Epoch 158/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003411185384005755 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.7288228571407842 accuracy 0.8157894736842105\n","\n","Epoch 159/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034366542988841786 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.732347711920113 accuracy 0.8157894736842105\n","\n","Epoch 160/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032554216577125317 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8322739154096723 accuracy 0.8070175438596491\n","\n","Epoch 161/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.002756244192338997 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8370391800996515 accuracy 0.8070175438596491\n","\n","Epoch 162/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003585623392019066 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8417279869314882 accuracy 0.8070175438596491\n","\n","Epoch 163/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00273231142927699 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8423964902755472 accuracy 0.8070175438596491\n","\n","Epoch 164/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0027941619504936455 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8480098098513338 accuracy 0.7982456140350876\n","\n","Epoch 165/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003490276786002333 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8531034514305134 accuracy 0.7982456140350876\n","\n","Epoch 166/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003029846371292777 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8567791208622282 accuracy 0.7982456140350876\n","\n","Epoch 167/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035168875018554147 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.86383240669943 accuracy 0.7982456140350876\n","\n","Epoch 168/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035357730229508277 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8671996966002098 accuracy 0.7982456140350876\n","\n","Epoch 169/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032804278855502744 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8745381236074081 accuracy 0.7982456140350876\n","\n","Epoch 170/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032376534950424585 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8812780156729332 accuracy 0.7982456140350876\n","\n","Epoch 171/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032572194168496438 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.88671247661091 accuracy 0.7982456140350876\n","\n","Epoch 172/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0029728816740401953 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.88467859476782 accuracy 0.7982456140350876\n","\n","Epoch 173/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035615826271007123 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8859686031935325 accuracy 0.7982456140350876\n","\n","Epoch 174/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003102320482242682 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8861700892446152 accuracy 0.7982456140350876\n","\n","Epoch 175/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036028386844142446 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8848314881322494 accuracy 0.7982456140350876\n","\n","Epoch 176/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0028563863771573494 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.898224636912147 accuracy 0.7982456140350876\n","\n","Epoch 177/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0031691390003620867 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.8944153487680637 accuracy 0.7982456140350876\n","\n","Epoch 178/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003677208586766234 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.900537952780553 accuracy 0.7982456140350876\n","\n","Epoch 179/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.004045159161553791 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9075734540818416 accuracy 0.7982456140350876\n","\n","Epoch 180/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0028684987894038727 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9097411781547748 accuracy 0.7982456140350876\n","\n","Epoch 181/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034121654344758667 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9113226458428585 accuracy 0.7982456140350876\n","\n","Epoch 182/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003487841094072408 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.915848664939233 accuracy 0.7982456140350876\n","\n","Epoch 183/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00357616123574811 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.922228060662576 accuracy 0.7982456140350876\n","\n","Epoch 184/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003584492600998246 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9252676144240581 accuracy 0.7982456140350876\n","\n","Epoch 185/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034956756460982064 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.926005952060052 accuracy 0.7982456140350876\n","\n","Epoch 186/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0036033931235506245 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9239426031707012 accuracy 0.7982456140350876\n","\n","Epoch 187/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0033897244767322476 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.921735987067052 accuracy 0.7982456140350876\n","\n","Epoch 188/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034925585397473924 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.92664723098261 accuracy 0.7982456140350876\n","\n","Epoch 189/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0030256525002236856 accuracy 0.9990234375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9279252290724145 accuracy 0.7982456140350876\n","\n","Epoch 190/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0035340317395711196 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9305388927458154 accuracy 0.7982456140350876\n","\n","Epoch 191/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032755867585283482 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9303075447677003 accuracy 0.7982456140350876\n","\n","Epoch 192/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0034202362432829148 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.931941159069396 accuracy 0.7982456140350876\n","\n","Epoch 193/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032692382182197832 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9300016239284048 accuracy 0.7982456140350876\n","\n","Epoch 194/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003631274051526745 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9308043420313368 accuracy 0.7982456140350876\n","\n","Epoch 195/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0032734293149534466 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.929820805787898 accuracy 0.7982456140350876\n","\n","Epoch 196/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.00240909341996165 accuracy 0.9990234375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.930096112191535 accuracy 0.7982456140350876\n","\n","Epoch 197/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.002924694481952983 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9303537532685766 accuracy 0.7982456140350876\n","\n","Epoch 198/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003460737614403797 accuracy 0.9990234375\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9310543164609442 accuracy 0.7982456140350876\n","\n","Epoch 199/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.003205547164752076 accuracy 0.998046875\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9310669377444754 accuracy 0.7982456140350876\n","\n","Epoch 200/200\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 0.0037124819821690025 accuracy 0.99853515625\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val   loss 1.9313213005660543 accuracy 0.7982456140350876\n","\n","CPU times: user 36min 30s, sys: 7min, total: 43min 30s\n","Wall time: 45min 30s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n9M3xqTv8Gzb","executionInfo":{"status":"ok","timestamp":1606941652901,"user_tz":300,"elapsed":15,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}}},"source":[""],"execution_count":211,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMrAsL6PtbxU","colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"status":"ok","timestamp":1606941653540,"user_tz":300,"elapsed":647,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"c2b3a4b0-bcb8-495b-c114-643cd05227c8"},"source":["## Plot Train and Val Accuracy\n","\n","plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"],"execution_count":212,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[],"image/png":{"width":729,"height":505}}}]},{"cell_type":"code","metadata":{"id":"1oySK_ph0CbS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606941654054,"user_tz":300,"elapsed":519,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"8da93963-57bf-45a5-d70b-9bca806a7b41"},"source":["## Evaluation\n","\n","test_acc, _ = eval_model(\n","  model,\n","  test_data_loader,\n","  loss_fn,\n","  device,\n","  len(df_test)\n",")\n","\n","test_acc.item()"],"execution_count":213,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.894736842105263"]},"metadata":{"tags":[]},"execution_count":213}]},{"cell_type":"code","metadata":{"id":"hoYeMP3F0ve1","executionInfo":{"status":"ok","timestamp":1606941654055,"user_tz":300,"elapsed":9,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}}},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  \n","  input_text = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      texts = d[\"input_text\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      probs = F.softmax(outputs, dim=1)\n","\n","      input_text.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(probs)\n","      real_values.extend(targets)\n","\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return input_text, predictions, prediction_probs, real_values"],"execution_count":214,"outputs":[]},{"cell_type":"code","metadata":{"id":"X69hhNmq0zwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606941654871,"user_tz":300,"elapsed":821,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"9394b7ef-006f-4e5f-fe59-eec47d962777"},"source":["y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  model,\n","  test_data_loader\n",")"],"execution_count":215,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"WvIslIhd0__F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606941654878,"user_tz":300,"elapsed":14,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"a9b03572-f32c-4dc7-ffb5-a6613d33a6a5"},"source":["class_names = [\"Non-COVID\",\"COVID\"]\n","print(classification_report(y_test, y_pred, target_names=class_names))"],"execution_count":216,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","   Non-COVID       0.85      0.92      0.88        50\n","       COVID       0.93      0.88      0.90        64\n","\n","    accuracy                           0.89       114\n","   macro avg       0.89      0.90      0.89       114\n","weighted avg       0.90      0.89      0.90       114\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a8wCjvP-2SgO","colab":{"base_uri":"https://localhost:8080/","height":539},"executionInfo":{"status":"ok","timestamp":1606941655210,"user_tz":300,"elapsed":343,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"81503bed-c75d-4e66-db46-bbcd24df2c81"},"source":["def show_confusion_matrix(confusion_matrix):\n","  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","  plt.ylabel('True Label')\n","  plt.xlabel('Predicted Label');\n","\n","cm = confusion_matrix(y_test, y_pred)\n","df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","show_confusion_matrix(df_cm)"],"execution_count":217,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 2 Axes>"]},"metadata":{"tags":[],"image/png":{"width":727,"height":522}}}]},{"cell_type":"code","metadata":{"id":"b3rxFbzX2knJ","executionInfo":{"status":"ok","timestamp":1606941752665,"user_tz":300,"elapsed":314,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}}},"source":["idx = 34\n","\n","review_text = y_review_texts[idx]\n","true_sentiment = y_test[idx]\n","pred_df = pd.DataFrame({\n","  'class_names': class_names,\n","  'values': y_pred_probs[idx]\n","})"],"execution_count":230,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjsvd0xV2swT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606941753666,"user_tz":300,"elapsed":610,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"2e6f7d72-aa61-48fa-b8e3-1719a363fbb4"},"source":["print(\"\\n\".join(wrap(review_text)))\n","print()\n","print(f'True sentiment: {class_names[true_sentiment]}')"],"execution_count":231,"outputs":[{"output_type":"stream","text":["This is sad. How is the Government of Uganda ensuring that people\n","observe the SOPs? Local leaders are not anywhere to enforce anything.\n","\n","True sentiment: COVID\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fGRW9HkX2u__","colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"status":"ok","timestamp":1606941761339,"user_tz":300,"elapsed":731,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"194fa9c8-2025-4766-e9e9-19bc72bafe40"},"source":["sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n","plt.ylabel('Label')\n","plt.xlabel('probability (Confidence)')\n","plt.xlim([0, 1]);"],"execution_count":232,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[],"image/png":{"width":790,"height":488}}}]},{"cell_type":"code","metadata":{"id":"vZQNze8s3CCk","executionInfo":{"status":"ok","timestamp":1606942609793,"user_tz":300,"elapsed":314,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}}},"source":["## Predicting on raw text \n","\n","review_text = \"Attention is all you need\""],"execution_count":371,"outputs":[]},{"cell_type":"code","metadata":{"id":"jM8l3Q5h3NJv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606942610120,"user_tz":300,"elapsed":243,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"7c3e3bf3-1f21-4ab7-ffd6-8a2bbcf9bc09"},"source":["encoded_review = tokenizer.encode_plus(\n","  review_text,\n","  max_length=MAX_LEN,\n","  add_special_tokens=True,\n","  return_token_type_ids=False,\n","  pad_to_max_length=True,\n","  return_attention_mask=True,\n","  return_tensors='pt',\n",")"],"execution_count":372,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Yuk2j4Pg3SLc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606942610642,"user_tz":300,"elapsed":368,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}},"outputId":"ccb030a6-62ed-411a-ec5f-8753aa4a8ccf"},"source":["input_ids = encoded_review['input_ids'].to(device)\n","attention_mask = encoded_review['attention_mask'].to(device)\n","\n","output = model(input_ids, attention_mask)\n","_, prediction = torch.max(output, dim=1)\n","\n","print(f'Review text: {review_text}')\n","print(f'Sentiment  : {class_names[prediction]}')"],"execution_count":373,"outputs":[{"output_type":"stream","text":["Review text: Attention is all you need\n","Sentiment  : Non-COVID\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BkpSu72u3UoN","executionInfo":{"status":"ok","timestamp":1606941655607,"user_tz":300,"elapsed":36,"user":{"displayName":"Benjamin Akera","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgJJ9jvhyJ5le4irZVcIhYTshtoJh05uj5fZnaw33U=s64","userId":"07485037250327296164"}}},"source":[""],"execution_count":223,"outputs":[]}]}